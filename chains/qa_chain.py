import os
from rich import print as pprint
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import Docx2txtLoader
from langchain.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers.string import StrOutputParser
from langchain_core.prompts.chat import ChatPromptTemplate
from langchain_core.runnables.passthrough import RunnablePassthrough
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from pydantic import BaseModel
from langchain.agents import (
    AgentExecutor, create_openai_tools_agent
)
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.tools import StructuredTool
from langchain_community.chat_models import ChatOllama
from pydantic import BaseModel
from chains.tools_api import fa_list_tool
from prompts.agent_prompt import agent_prompt
from prompts.qa_fewshot_prompt import few_shot_prompt
class SCFQAInput(BaseModel):
    input: str
    role: str
    token: str
    session_id: str
# è¼‰å…¥ .env æª”
load_dotenv()

# å–å¾— GOOGLE_API_KEY
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    raise ValueError("è«‹åœ¨ .env æª”ä¸­è¨­å®š GOOGLE_API_KEY")

# è¨­å®šç’°å¢ƒè®Šæ•¸ï¼ˆå¦‚æœä½ å¾ŒçºŒç”¨çš„ç¨‹å¼éœ€è¦ï¼‰
os.environ["GOOGLE_API_KEY"] = api_key

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
# llm = ChatOllama(
#     model="gemma3:12b",  # æ¨¡å‹åç¨±
#     base_url="http://192.168.2.255:11434",  # ä½ æä¾›çš„ IP å’Œ port
# )
# llm = ChatOllama(
#     model="deepseek-r1:14b",  # æ¨¡å‹åç¨±
#     base_url="http://59.126.229.182:11434",  # ä½ æä¾›çš„ IP å’Œ port
# )

# åµŒå…¥æ¨¡å‹
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

loader = Docx2txtLoader("./scf2.0_chatbot.docx")
docs = loader.load()
# åˆä½µæˆä¸€å€‹å¤§ Document
full_text = "\n".join([doc.page_content for doc in docs])
word_docs = Document(page_content=full_text)
# pprint(word_docs)


text_splitter = RecursiveCharacterTextSplitter(separators=["[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€"],
                                                is_separator_regex=True,
                                                chunk_size=300,
                                                chunk_overlap=20)
splits = text_splitter.split_documents([word_docs])
# pprint(splits)

# å»ºç«‹å‘é‡è³‡æ–™åº«ï¼Œå°‡ splits åˆ‡å¥½çš„æ–‡ä»¶ embed ä¸¦å„²å­˜
db = FAISS.from_documents(splits, embeddings)

# å„²å­˜å‘é‡è³‡æ–™åº«åˆ°æœ¬åœ°ç›®éŒ„
db.save_local(
    folder_path="./scf2.0_db",     # å„²å­˜è³‡æ–™å¤¾
    index_name="scf2.0_test1"      # å„²å­˜æª”åå‰ç¶´
)

# å¾æœ¬åœ°é‡æ–°è¼‰å…¥å‘é‡è³‡æ–™åº«
new_db = FAISS.load_local(
    folder_path="./scf2.0_db",
    index_name="scf2.0_test1",
    embeddings=embeddings,
    allow_dangerous_deserialization=True  # è‹¥é‡ pickle è¼‰å…¥è­¦å‘Šéœ€åŠ ä¸Š
)


str_parser = StrOutputParser()
system_template = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ã€èè³‡å¹³å°å•ç­”åŠ©ç†ã€‘ã€‚

ã€é‡è¦æŒ‡ç¤ºã€‘
- è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å’Œæ¨™æº–å›æ‡‰ï¼Œç¢ºä¿æ¯æ¬¡å›æ‡‰çš„ä¸€è‡´æ€§
- å…ˆåƒè€ƒå›è¦†çš„ç¤ºç¯„ï¼Œä¸¦åƒ…å›æ‡‰æ–‡å­—
- ä½¿ç”¨è€…è§’è‰²æ˜¯ {role}ï¼Œè‹¥ {role} ç‚º 'Bank'ï¼Œä»£è¡¨è§’è‰²æ˜¯'éŠ€è¡Œ'; 'Buyer' ä»£è¡¨'ä¸­å¿ƒå» '; 'Supplier' ä»£è¡¨'ä¾›æ‡‰å•†'

ã€å›æ‡‰è¦å‰‡ã€‘
1. è§’è‰²é™åˆ¶ï¼šæ‰€æœ‰å›ç­”åƒ…é‡å° {role} è§’è‰²å¯åŸ·è¡Œçš„æ“ä½œæˆ–æ¬Šé™ï¼Œä¸å¯æåŠå…¶ä»–è§’è‰²çš„æµç¨‹æˆ–æ“ä½œï¼Œè‹¥è©²è§’è‰²ç„¡æ¬Šé™ï¼Œè«‹å›æ‡‰æ‚¨ä¸¦ç„¡ç›¸é—œæ“ä½œæ¬Šé™
2. èªè¨€ï¼šä½¿ç”¨ç¹é«”ä¸­æ–‡ä½œç­”
3. æ ¼å¼ï¼šç´”æ–‡å­—æ ¼å¼ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½• Markdown èªæ³•ï¼ˆä¾‹å¦‚ **ã€*ã€`ã€-ã€> ç­‰ç¬¦è™Ÿï¼‰ã€‚
4. çµæ§‹ï¼š
- æ­¥é©Ÿèªªæ˜ï¼šä½¿ç”¨æ•¸å­—æ¢åˆ— (1. 2. 3.)
- åŠŸèƒ½åˆ—èˆ‰ï¼šä½¿ç”¨åœ“é»ç¬¦è™Ÿ (â€¢)
- æ¯å€‹æ­¥é©Ÿæˆ–åŠŸèƒ½é»æ‡‰ç°¡æ½”æ˜ç¢º
5. å…§å®¹æº–ç¢ºæ€§ï¼šå›è¦†å…§å®¹å‹™å¿…èˆ‡æ–‡ä»¶å…§å®¹ä¸€è‡´ï¼Œè‹¥æ–‡ä»¶ä¸­æœªæåŠï¼Œè«‹æ˜ç¢ºèªªã€ŒæŠ±æ­‰ï¼Œæ‚¨æ‰€æçš„å•é¡Œå¯èƒ½èˆ‡ SCF å¹³å°ç„¡é—œï¼Œæˆ–æ‚¨ç›®å‰è§’è‰²ç„¡ç›¸é—œæ“ä½œæ¬Šé™ã€‚ã€

ã€æ¨™æº–å›æ‡‰æ ¼å¼ç¯„ä¾‹ã€‘
å•é¡Œé¡å‹ï¼šæ“ä½œæµç¨‹
æ ¼å¼ï¼š
è«‹ä¾ä»¥ä¸‹æ­¥é©Ÿé€²è¡Œï¼š
1. ç¬¬ä¸€æ­¥é©Ÿ
2. ç¬¬äºŒæ­¥é©Ÿ
3. ç¬¬ä¸‰æ­¥é©Ÿ

å•é¡Œé¡å‹ï¼šæ¬Šé™/åŠŸèƒ½æŸ¥è©¢
æ ¼å¼ï¼š
{role}è§’è‰²æ¬Šé™åŒ…æ‹¬ï¼š
â€¢ åŠŸèƒ½ä¸€
â€¢ åŠŸèƒ½äºŒ
â€¢ åŠŸèƒ½ä¸‰


ã€éç›¸é—œå•é¡Œæ¨™æº–å›æ‡‰ã€‘
è‹¥å•é¡Œèˆ‡èè³‡å¹³å°ç„¡é—œï¼Œè«‹å›è¦†ï¼š
ã€Œå¾ˆæŠ±æ­‰ï¼Œæˆ‘åªèƒ½å›ç­”æœ‰é—œ èè³‡å¹³å° ç›¸é—œçš„å•é¡Œã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—œæ–¼èè³‡å¹³å°çš„å•é¡Œï¼Œæ­¡è¿éš¨æ™‚æå•ï¼ã€

[æ–‡ä»¶å…§å®¹]
{context}

[å•é¡Œ]
{input}
"""

# qa_prompt = ChatPromptTemplate.from_template(system_template)
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", system_template),
    *few_shot_prompt.format_messages(),
    ("human", "ä½¿ç”¨è€…è§’è‰²æ˜¯ {role}ï¼Œå•é¡Œæ˜¯ï¼š{input}")
])

# å»ºç«‹æª¢ç´¢å™¨
retriever = new_db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# retriever åªæ¥å—ä¸€å€‹å­—ä¸² queryï¼Œä¸æ¥å— dict
def retriever_func(inputs):
    input = inputs["input"]
    return retriever.get_relevant_documents(input)
chain = (
    {"context": retriever_func, "input": RunnablePassthrough(),"role": RunnablePassthrough()}
    | qa_prompt
    | llm
    | str_parser
)

# å·¥å…·
def scf_qa_chain_run(query: str) -> str:
    return chain.invoke(query)

def scf_qa_chain_run_stream(input: str, role: str, token: str, session_id: str) -> str:
    response = ""
    
    for chunk in chain.stream({"input": input, "role": role, "token":token, "session_id":session_id}):
        response += chunk
    
    return response

qa_tool = StructuredTool.from_function(
    func=scf_qa_chain_run_stream,
    name="SCF_QA",
    description=(
        "å›ç­”ä½¿ç”¨è€…é—œæ–¼ SCF å¹³å°æ“ä½œæµç¨‹çš„å•é¡Œ"
        "ä¾‹å¦‚ï¼šå¦‚ä½•ä¿®æ”¹å¯†ç¢¼ï¼Ÿå¦‚ä½•å¯©æ ¸æ¡ˆä»¶ï¼Ÿå¦‚ä½•ä¸Šå‚³æ¡ˆä»¶ï¼Ÿæ¡ˆä»¶æµç¨‹ï¼Ÿå¦‚ä½•é€å‡ºç”³è«‹ï¼Ÿæ¬Šé™æœ‰å“ªäº›ï¼Ÿ"
        "è«‹åœ¨ä½ åˆ¤æ–·èˆ‡ SCF æ“ä½œæµç¨‹ç›¸é—œæ™‚ä½¿ç”¨é€™å€‹å·¥å…·"
    ),
)
tools = [qa_tool,fa_list_tool]

store = {}

# æ ¹æ“š session_id å»ºç«‹æˆ–å–å¾—å°æ‡‰çš„å°è©±æ­·å²è¨˜éŒ„ï¼ˆè¨˜æ†¶ï¼‰
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def print_session_history(session_id: str):
    if session_id in store:
        print(f"ğŸ“š Session: {session_id} è¨˜æ†¶å…§å®¹å¦‚ä¸‹ï¼š")
        for i, msg in enumerate(store[session_id].messages):
            print(f"{i+1}. [{msg.type}] {msg.content}")
    else:
        print(f"âŒ æŸ¥ç„¡ sessionï¼š{session_id}")



# æ¸…é™¤æ­·å²ç´€éŒ„
def clear_session_history(session_id: str):
    if session_id in store:
        del store[session_id]
        print(f"âœ… Session '{session_id}' çš„è¨˜æ†¶å·²è¢«æ¸…é™¤ã€‚")
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ° session '{session_id}'ï¼Œç„¡éœ€æ¸…é™¤ã€‚")

agent = create_openai_tools_agent(llm, tools, agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools,verbose = True)
# å»ºç«‹æœ‰è¨˜æ†¶èƒ½åŠ›çš„ Agent Chain
agent_with_chat_history = RunnableWithMessageHistory(
    agent_executor,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)
